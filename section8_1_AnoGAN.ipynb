{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mag381/dog/blob/main/section8_1_AnoGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRpoQRd7pYOG"
      },
      "source": [
        "# Download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibP7THZhpYOM",
        "outputId": "1a90acb9-91d3-41a0-9814-d3113953edbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-19 02:02:48--  https://md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com/rp73yg93n8-1.zip\n",
            "Resolving md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com (md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com)... 3.5.69.106\n",
            "Connecting to md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com (md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com)|3.5.69.106|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 418811602 (399M) [application/octet-stream]\n",
            "Saving to: ‘./data/rp73yg93n8-1.zip’\n",
            "\n",
            "rp73yg93n8-1.zip    100%[===================>] 399.41M  22.4MB/s    in 18s     \n",
            "\n",
            "2022-05-19 02:03:07 (22.0 MB/s) - ‘./data/rp73yg93n8-1.zip’ saved [418811602/418811602]\n",
            "\n",
            "Archive:  ./data/rp73yg93n8-1.zip\n",
            "  inflating: ./data/fruits-360_dataset.zip  \n"
          ]
        }
      ],
      "source": [
        "# Fruits-360 dataset をダウンロードし、解凍する\n",
        "\n",
        "!wget https://md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com/rp73yg93n8-1.zip -nc -P ./data/\n",
        "!unzip -n ./data/rp73yg93n8-1.zip -d ./data/\n",
        "!unzip -n -q ./data/fruits-360_dataset.zip -d ./data/ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Inj8u_Q9pYON"
      },
      "source": [
        "# Import "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gG_d__VZpYON"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from glob import glob\n",
        "from warnings import filterwarnings\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.init as init\n",
        "from natsort import natsorted\n",
        "from PIL import Image\n",
        "from skimage import io, transform\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid, save_image\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "filterwarnings(\"ignore\")  # warningをオフにする"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68lzQjLipYON"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Htc00RaFpYOO"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 96  # 画像の読み込みサイズ\n",
        "EMBED_SIZE = 128  # 潜在変数zの次元数\n",
        "BATCH_SIZE = 16  # バッチサイズ\n",
        "EPOCHS = 1000  # エポック数\n",
        "LR = 0.0004  # 学習率"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Raz3X7NapYOO"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # GPUが使えるならGPUで、そうでないならCPUで計算する\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4PCPvxWqpYOO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9846231e-126a-41f7-b1b1-fa1bc3905745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "COLAB_FLG = True  # COLABで実行する場合はTrue, それ以外で実行する場合はFalse\n",
        "\n",
        "if COLAB_FLG:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive') # マウント先\n",
        "    ATTACH_PATH = \"/content/gdrive/MyDrive/Colab Notebooks/gan_sample/chapter8\"  # 保存先のベースディレクトリ\n",
        "else:\n",
        "    ATTACH_PATH = \".\"  # ローカルならカレントディレクトリ\n",
        "    \n",
        "    \n",
        "SAVE_MODEL_PATH = f\"{ATTACH_PATH}/results/AnoGAN/model/\"  # モデルの保存先\n",
        "SAVE_IMAGE_FROM_Z_PATH = f\"{ATTACH_PATH}/results/AnoGAN/image/image_from_z/\"  # 乱数から生成した画像の保存先\n",
        "\n",
        "# 保存先のディレクトリを作成する\n",
        "os.makedirs(SAVE_MODEL_PATH, exist_ok=True)  \n",
        "os.makedirs(SAVE_IMAGE_FROM_Z_PATH, exist_ok=True)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4KSxYpHpYOO"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1IgSGsYpYOO"
      },
      "outputs": [],
      "source": [
        "train_root = './data/fruits-360/Training/Physalis/'  # train dataの保存してあるディレクトリ\n",
        "val_root = './data/fruits-360/Test/Physalis/'  # val dataの保存してあるディレクトリ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyeobAE7pYOP"
      },
      "outputs": [],
      "source": [
        "# ディレクトリから画像を読み込んでDataLoaderに渡す用のクラス\n",
        "\n",
        "class LoadFromFolder(Dataset):\n",
        "    def __init__(self, main_dir, transform):\n",
        "        self.main_dir = main_dir\n",
        "        self.transform = transform\n",
        "        all_imgs = natsorted(os.listdir(main_dir))\n",
        "        self.all_imgs_name = natsorted(all_imgs)\n",
        "        self.imgs_loc = [os.path.join(self.main_dir, i) for i in self.all_imgs_name]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_imgs_name)\n",
        "    \n",
        "    def load_image(self, path):\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "        tensor_image = self.transform(image)\n",
        "        return tensor_image\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        # 後ほどsliceで画像を複数枚取得したいのでsliceでも取れるようにする\n",
        "        if type(idx) == slice:\n",
        "            paths = self.imgs_loc[idx]\n",
        "            tensor_image = [self.load_image(path) for path in paths]\n",
        "            tensor_image = torch.cat(tensor_image).reshape(len(tensor_image), *tensor_image[0].shape)\n",
        "        elif type(idx) == int:\n",
        "            path = self.imgs_loc[idx]\n",
        "            tensor_image = self.load_image(path)\n",
        "        return tensor_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXMJyVqWpYOP"
      },
      "outputs": [],
      "source": [
        "# 画像を読み込む際の前処理\n",
        "\n",
        "transform_dict = {\n",
        "    \"train\": transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),  # IMAGE_SIZEにreshape\n",
        "            transforms.RandomHorizontalFlip(), # ランダムに左右反転を行う\n",
        "            transforms.ToTensor(),\n",
        "        ]\n",
        "    ),\n",
        "    \"test\": transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),  # IMAGE_SIZEにreshape\n",
        "            transforms.ToTensor(),\n",
        "        ]\n",
        "    ),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iujdzoLpYOP"
      },
      "outputs": [],
      "source": [
        "# 読み込む\n",
        "train_dataset = LoadFromFolder(train_root, transform=transform_dict[\"train\"])\n",
        "\n",
        "test_dataset = LoadFromFolder(val_root, transform=transform_dict[\"test\"])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size = BATCH_SIZE, shuffle=True, **kwargs)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size = BATCH_SIZE, shuffle=True, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbeqYYo9pYOP"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCcFI0U8pYOP"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, EMBED_SIZE=EMBED_SIZE):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(EMBED_SIZE, 256, kernel_size=6, stride=1, padding=0, bias=False), # 6x6\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False), # 12x12\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False), # 24x24\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1, bias=False), # 48x48\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1, bias=False), #96x96\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        out = self.main(z)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJHeEolSpYOQ"
      },
      "outputs": [],
      "source": [
        "# ネットワークを可視化する\n",
        "\n",
        "summary(Generator().to(device), tuple([EMBED_SIZE, 1, 1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z67tSZoEpYOQ"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.LeakyReLU(0.1, inplace=True), #48x48\n",
        "            nn.Dropout2d(p=0.3),\n",
        "            \n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.1, inplace=True), #24x24\n",
        "            nn.Dropout2d(p=0.3),\n",
        "            \n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.1, inplace=True), #12x12\n",
        "            nn.Dropout2d(p=0.3),\n",
        "            \n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.1, inplace=True), #6x6\n",
        "            nn.Dropout2d(p=0.3),\n",
        "\n",
        "        )\n",
        "        self.last = nn.Sequential(\n",
        "            nn.Conv2d(256, 1, kernel_size=6, stride=1, padding=0, bias=False) # 1x1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        feature = self.main(x)\n",
        "        out = self.last(feature)\n",
        "        out = F.sigmoid(out)\n",
        "        feature = feature.view(feature.size()[0], -1)\n",
        "        out = out.squeeze()\n",
        "        return out, feature\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsuIX1R2pYOQ"
      },
      "outputs": [],
      "source": [
        "# ネットワークを可視化する\n",
        "\n",
        "summary(Discriminator().to(device), (3, IMAGE_SIZE, IMAGE_SIZE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXZVejaXpYOQ"
      },
      "outputs": [],
      "source": [
        "# 重みの初期化を行う関数\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTUpGEjVpYOQ"
      },
      "outputs": [],
      "source": [
        "model_G = Generator().to(device)\n",
        "model_G.apply(weights_init)\n",
        "\n",
        "model_D = Discriminator().to(device)\n",
        "model_D.apply(weights_init)\n",
        "\n",
        "criterion = nn.BCELoss()  # 評価関数\n",
        "optimizer_g = torch.optim.Adam(model_G.parameters(), lr= LR,betas=(0.5,0.999))  # Generatorのoptimizer\n",
        "optimizer_d = torch.optim.Adam(model_D.parameters(), lr= LR,betas=(0.5,0.999))  # Discriminatorのoptimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8muljE1pYOR"
      },
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjJ_RyGFpYOR",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "loss_d_list, loss_g_list = [], []\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    loss_d_sum = 0\n",
        "    loss_g_sum = 0\n",
        "    \n",
        "    for i,(x, x_val) in enumerate(zip(train_loader, val_loader)):\n",
        "        \n",
        "        model_G.train()\n",
        "        model_D.train()\n",
        "        \n",
        "        # set values\n",
        "        y_true = Variable(torch.ones(x.size()[0])).to(device)\n",
        "        y_fake = Variable(torch.zeros(x.size()[0])).to(device)\n",
        "        \n",
        "        x = Variable(x).to(device)\n",
        "        z = Variable(init.normal(torch.Tensor(x.size()[0],EMBED_SIZE, 1, 1),mean=0,std=0.1)).to(device)\n",
        "        \n",
        "\n",
        "        # discriminator\n",
        "        optimizer_d.zero_grad()\n",
        "        \n",
        "        G_z = model_G(z)\n",
        "        p_true, _ = model_D(x)\n",
        "        p_fake, _ = model_D(G_z)\n",
        "        \n",
        "        loss_d = criterion(p_true, y_true) + criterion(p_fake, y_fake)\n",
        "        loss_d.backward(retain_graph=True)\n",
        "        optimizer_d.step()\n",
        "        \n",
        "        # generator and encoder\n",
        "        optimizer_g.zero_grad()\n",
        "        \n",
        "        p_true, _ = model_D(x)\n",
        "        p_fake, _ = model_D(G_z)        \n",
        "        \n",
        "        loss_g = criterion(p_fake, y_true) + criterion(p_true, y_fake)\n",
        "        loss_g.backward(retain_graph=True)\n",
        "        optimizer_g.step()\n",
        "        \n",
        "        \n",
        "        loss_d_sum += loss_d.item()\n",
        "        loss_g_sum += loss_g.item()\n",
        "        \n",
        "            \n",
        "        # save images\n",
        "        if i == 0:\n",
        "            \n",
        "            model_G.eval()\n",
        "            model_D.eval()\n",
        "        \n",
        "            save_image_size_for_z = min(BATCH_SIZE, 8)\n",
        "            save_images = model_G(z)\n",
        "            save_image(save_images[:save_image_size_for_z], f\"{SAVE_IMAGE_FROM_Z_PATH}/epoch_{epoch}.png\", nrow=4)\n",
        "\n",
        "        \n",
        "        \n",
        "    # record loss\n",
        "    loss_d_mean = loss_d_sum / len(train_loader)\n",
        "    loss_g_mean = loss_g_sum / len(train_loader)\n",
        "    \n",
        "    print(f\"{epoch}/{EPOCHS} epoch g_loss: {loss_g_mean:.3f} d_loss: {loss_d_mean:.3f}\")\n",
        "    \n",
        "    loss_d_list.append(loss_d_mean)\n",
        "    loss_g_list.append(loss_g_mean)\n",
        "    \n",
        "    # save model\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        torch.save(model_G.state_dict(),f'{SAVE_MODEL_PATH}/Generator_{epoch + 1}.pkl')\n",
        "        torch.save(model_D.state_dict(),f'{SAVE_MODEL_PATH}/Discriminator_{epoch + 1}.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybIcdyMOpYOR"
      },
      "outputs": [],
      "source": [
        "# GeneratorとDiscriminatorのLossの推移\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.plot(range(len(loss_g_list)), loss_g_list, label=\"g loss\")\n",
        "plt.plot(range(len(loss_d_list)), loss_d_list, label=\"d loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w5h7x8TpYOR"
      },
      "source": [
        "# Latent Z optimization and Test anomaly detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMggjpxdpYOS"
      },
      "outputs": [],
      "source": [
        "# 異常度を測定する関数\n",
        "\n",
        "criterion_L1 = nn.L1Loss(reduction=\"sum\")\n",
        "\n",
        "def Anomaly_score(x,G_z,Lambda=0.1):\n",
        "    _,x_feature = model_D(x)\n",
        "    _,G_z_feature = model_D(G_z)\n",
        "    \n",
        "    residual_loss = criterion_L1(x, G_z)  \n",
        "    discrimination_loss = criterion_L1(x_feature, G_z_feature)\n",
        "    total_loss = (1-Lambda)*residual_loss + Lambda*discrimination_loss\n",
        "    \n",
        "    return total_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCpWMIM7pYOS",
        "outputId": "bf16db3d-1bb4-46d1-cfc8-9aad1f183003"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load model\n"
          ]
        }
      ],
      "source": [
        "# 学習したモデルの読み込み\n",
        "\n",
        "LOAD_EPOCH = 1000\n",
        "\n",
        "model_G = Generator().to(device)\n",
        "model_G.load_state_dict(torch.load(f\"{SAVE_MODEL_PATH}/Generator_{LOAD_EPOCH}.pkl\"))\n",
        "model_G.eval()\n",
        "\n",
        "\n",
        "model_D = Discriminator().to(device)\n",
        "model_D.load_state_dict(torch.load(f\"{SAVE_MODEL_PATH}/Discriminator_{LOAD_EPOCH}.pkl\"))\n",
        "model_D.eval()\n",
        "\n",
        "print(\"load model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ODHNpGqpYOS"
      },
      "outputs": [],
      "source": [
        "# 最適なzを探索する関数\n",
        "def optimize_z(x):\n",
        "    x = Variable(x).to(device)\n",
        "    z = Variable(init.normal(torch.zeros(1,EMBED_SIZE, 1, 1),mean=0,std=0.1).to(device),requires_grad=True)\n",
        "    z_optimizer = torch.optim.Adam([z],lr=1e-4)\n",
        "    \n",
        "    for i in range(1000):\n",
        "        G_z = model_G(z)\n",
        "\n",
        "        loss = Anomaly_score(x, G_z)\n",
        "        loss.backward()\n",
        "        z_optimizer.step()\n",
        "            \n",
        "    return z\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROW7FcgVpYOS",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# 正常な画像で実行\n",
        "\n",
        "random_image_size = 10\n",
        "\n",
        "test_root_normal = './data/fruits-360/Test/Physalis/'\n",
        "test_dataset_normal = LoadFromFolder(test_root_normal, transform=transform_dict[\"test\"])\n",
        "\n",
        "test_images_normal = random.sample(list(test_dataset_normal), random_image_size)\n",
        "\n",
        "# うまく再現され、異常スコアが低くなっていれば成功\n",
        "for idx in range(len(test_images_normal)):\n",
        "\n",
        "    x = test_images_normal[idx].view(1, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
        "    x = Variable(x).to(device)\n",
        "    \n",
        "    z_o = optimize_z(x)\n",
        "    G_z_o = model_G(z_o)\n",
        "    loss = Anomaly_score(x, G_z_o)\n",
        "    diff_img = torch.abs(x - G_z_o)\n",
        "\n",
        "    print(f\"Anomary_score = {loss.cpu().data:.3f}\")\n",
        "    comparison = torch.cat([x.to(\"cpu\"), G_z_o.to(\"cpu\"), diff_img.to(\"cpu\")])\n",
        "    joined_image = make_grid(comparison, nrow=3).detach().numpy()\n",
        "    joined_image = np.transpose(joined_image, [1, 2, 0])\n",
        "    \n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.imshow((joined_image * 255).astype(np.uint8))\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9THEdxgpYOS",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# 異常な画像で実行\n",
        "\n",
        "random_image_size = 10\n",
        "\n",
        "test_root_anomaly = './data/fruits-360/Test/Apple Braeburn/'\n",
        "test_dataset_anomaly = LoadFromFolder(test_root_anomaly, transform=transform_dict[\"test\"])\n",
        "\n",
        "test_images_anomaly = random.sample(list(test_dataset_anomaly), random_image_size)\n",
        "\n",
        "\n",
        "# うまく再現されず、異常スコアが高くなっていれば成功\n",
        "for idx in range(len(test_images_anomaly)):\n",
        "\n",
        "    x = test_images_anomaly[idx].view(1, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
        "    x = Variable(x).to(device)\n",
        "    \n",
        "    z_o = optimize_z(x)\n",
        "    G_z_o = model_G(z_o)\n",
        "    loss = Anomaly_score(x, G_z_o)\n",
        "    diff_img = torch.abs(x - G_z_o)\n",
        "\n",
        "    print(f\"Anomary_score = {loss.cpu().data:.3f}\")\n",
        "    comparison = torch.cat([x.to(\"cpu\"), G_z_o.to(\"cpu\"), diff_img.to(\"cpu\")])\n",
        "    joined_image = make_grid(comparison, nrow=3).detach().numpy()\n",
        "    joined_image = np.transpose(joined_image, [1, 2, 0])\n",
        "    \n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.imshow((joined_image * 255).astype(np.uint8))\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeGGj7WPpYOT",
        "outputId": "f7bf208c-d900-4ad0-b400-9927621c837a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "add damage\n"
          ]
        }
      ],
      "source": [
        "# 画像に傷を模した記号を付与する関数\n",
        "\n",
        "def add_damage(image_path):\n",
        "    \n",
        "    folder = os.path.dirname(image_path)\n",
        "    save_folder = folder + \"_damaged\"\n",
        "    save_base_path = os.path.basename(image_path).replace(\".jpg\", \"_damaged.jpg\")\n",
        "    save_path = os.path.join(save_folder, save_base_path)\n",
        "    \n",
        "    os.makedirs(save_folder, exist_ok=True)\n",
        "    \n",
        "    image = cv2.imread(image_path)\n",
        "    center_x = random.randint(20, 76)\n",
        "    center_y = random.randint(20, 76)\n",
        "    color_r = random.randint(0, 255)\n",
        "    color_g = random.randint(0, 255)\n",
        "    color_b = random.randint(0, 255)\n",
        "    \n",
        "    center = (center_x, center_y)\n",
        "    color = (color_r, color_g, color_b)\n",
        "    \n",
        "    cv2.circle(image, center = center, radius = 10, color = color,thickness=-1)\n",
        "    cv2.imwrite(save_path, image)\n",
        "    \n",
        "images_path = glob('./data/fruits-360/Test/Physalis/*.jpg')\n",
        "[add_damage(image_path) for image_path in images_path]\n",
        "print(\"add damage\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyVvjh4gpYOT",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# 異常な画像で実行\n",
        "\n",
        "test_root_anomaly = './data/fruits-360/Test/Physalis_damaged/'\n",
        "test_dataset_anomaly = LoadFromFolder(test_root_anomaly, transform=transform_dict[\"test\"])\n",
        "\n",
        "test_images_anomaly = random.sample(list(test_dataset_anomaly), random_image_size)\n",
        "\n",
        "# うまく再現されず、異常スコアが高くなっていれば成功\n",
        "for idx in range(len(test_images_anomaly)):\n",
        "\n",
        "    x = test_images_anomaly[idx].view(1, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
        "    x = Variable(x).to(device)\n",
        "    \n",
        "    z_o = optimize_z(x)\n",
        "    G_z_o = model_G(z_o)\n",
        "    loss = Anomaly_score(x, G_z_o)\n",
        "    diff_img = torch.abs(x - G_z_o)\n",
        "\n",
        "    print(f\"Anomary_score = {loss.cpu().data:.3f}\")\n",
        "    comparison = torch.cat([x.to(\"cpu\"), G_z_o.to(\"cpu\"), diff_img.to(\"cpu\")])\n",
        "    joined_image = make_grid(comparison, nrow=3).detach().numpy()\n",
        "    joined_image = np.transpose(joined_image, [1, 2, 0])\n",
        "    \n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.imshow((joined_image * 255).astype(np.uint8))\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fw22ebivpYOT"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "section8_1-AnoGAN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}